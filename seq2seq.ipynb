{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../pubmed/\"\n",
    "TEXT_DIR = \"train_sections.txt\"\n",
    "SUMM_DIR = \"train_summary_processed.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set hyper parameters\n",
    "MAX_NUM_WORDS = 4000 #vocab_size\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_UNITS = 32\n",
    "VAL_SPLIT = 0.1\n",
    "ENCODER_MAX_LEN = 500 #for one section\n",
    "DECODER_MAX_LEN = 150\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 10\n",
    "MODEL_NAME = \"seq2seq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# read dataset\n",
    "# first model: treat each section as one input for the encoder\n",
    "X = []\n",
    "with open(DATA_DIR+TEXT_DIR, 'r') as f:\n",
    "    f_l = list(f)\n",
    "    for line in f_l:\n",
    "        if line != None:\n",
    "            line2list = eval(line)\n",
    "            for sec in range(len(line2list)):\n",
    "                line2list[sec] = ' '.join(line2list[sec])\n",
    "            X.append(line2list)\n",
    "print (len(X))\n",
    "\n",
    "\n",
    "Y = []\n",
    "with open(DATA_DIR+SUMM_DIR, 'r') as f:\n",
    "    f_l = list(f)\n",
    "    for line in f_l:\n",
    "        if line != None:\n",
    "            Y.append(line)\n",
    "print (len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 is reserved fot padding, 1 for <UNK>, word idx starts from 2\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=1)\n",
    "# must feed in a list of list of strings\n",
    "all_words = [sec for sec in line for line in X]\n",
    "all_words.extend(Y)\n",
    "tokenizer.fit_on_texts(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4099 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print (\"Found %s unique tokens.\" % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add <sos>, <eos> token, <UNK>(OOV) is idx 0 by default\n",
    "vocab_word_index = {}\n",
    "vocab_index_word = {}\n",
    "for word, idx in word_index.items():\n",
    "    if idx <= MAX_NUM_WORDS - 2:\n",
    "        vocab_word_index[word] = idx\n",
    "        vocab_index_word[idx] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_word_index['<sos>'] = MAX_NUM_WORDS - 1\n",
    "vocab_index_word[MAX_NUM_WORDS] = '<sos>'\n",
    "vocab_word_index['<eos>'] = MAX_NUM_WORDS \n",
    "vocab_index_word[MAX_NUM_WORDS] = '<eos>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_IDX = MAX_NUM_WORDS\n",
    "SOS_IDX = MAX_NUM_WORDS - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert words to indices\n",
    "#Note: here we pad each section and use section for encoder\n",
    "#Another alternative is to use sentence as encoder and pad each sentence\n",
    "# note: encoder does not need <sos> or <eos>\n",
    "# for encoder input, pad in front\n",
    "# decoder input, pad in the end\n",
    "# note that number of sections is not constant for every article\n",
    "for i in range(len(X)):\n",
    "    X[i] = tokenizer.texts_to_sequences(X[i])\n",
    "    X[i] = pad_sequences(X[i], ENCODER_MAX_LEN, padding='pre')\n",
    "\n",
    "Y = tokenizer.texts_to_sequences(Y)\n",
    "for line in Y:\n",
    "    line.append(EOS_IDX)\n",
    "Y = pad_sequences(Y, DECODER_MAX_LEN, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_samples = len(X)\n",
    "# x_train = X[:-int(n_samples*VAL_SPLIT)]\n",
    "# y_train = Y[:-int(n_samples*VAL_SPLIT)]\n",
    "\n",
    "# x_val = X[-int(n_samples*VAL_SPLIT):]\n",
    "# y_val = Y[-int(n_samples*VAL_SPLIT):]\n",
    "\n",
    "y_train = np.asarray(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 150)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use pretrained 50d glove vectors\n",
    "#with larger dataset, we can try glove woith higher dimension or \n",
    "#learn word embedding from scratch\n",
    "# vector for UNK and those not in glove are randomly initialized\n",
    "GLOVE_DIR = \"glove.6B.50d.txt\"\n",
    "f_emb = open(GLOVE_DIR, 'r')\n",
    "embedding_index = {}\n",
    "for line in f_emb:\n",
    "    line = line.strip().split()\n",
    "    word = line[0]\n",
    "    coefs = np.asarray(line[1:], dtype='float32')\n",
    "    embedding_index[word] = coefs\n",
    "f_emb.close()\n",
    "\n",
    "embedding_matrix = np.random.random((MAX_NUM_WORDS+1, EMBEDDING_DIM))\n",
    "for word, i in vocab_word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the seq2seq model\n",
    "#add attention, pointer and coverage later\n",
    "#greedy search for now, use beam search later\n",
    "embedding_layer = layers.Embedding(MAX_NUM_WORDS+1, EMBEDDING_DIM, \n",
    "                                   weights=[embedding_matrix],\n",
    "                                   trainable=True)\n",
    "\n",
    "one_section_input = layers.Input(shape=(ENCODER_MAX_LEN,))\n",
    "one_section_embed = embedding_layer(one_section_input)\n",
    "one_section_lstm = layers.Bidirectional(layers.LSTM(HIDDEN_UNITS))(one_section_embed)\n",
    "one_section_encoder = Model(one_section_input, one_section_lstm)\n",
    "\n",
    "all_section_input = layers.Input(shape=(None,ENCODER_MAX_LEN,))\n",
    "all_section_embed = layers.TimeDistributed(one_section_encoder)(all_section_input)\n",
    "all_section_lstm = layers.LSTM(HIDDEN_UNITS, return_state=True)\n",
    "encoder_outputs, encoder_state_h, encoder_state_c = all_section_lstm(all_section_embed)\n",
    "\n",
    "encoder_states = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "# in training this is the summary, in inference this is the previous word\n",
    "# personally i feel that the decoder input should also be word vectors\n",
    "decoder_input = layers.Input(shape=(DECODER_MAX_LEN,))\n",
    "decoder_embed = embedding_layer(decoder_input)\n",
    "decoder_lstm = layers.LSTM(HIDDEN_UNITS, return_state=True, return_sequences=True)\n",
    "decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_embed, initial_state=encoder_states)\n",
    "decoder_dense = layers.TimeDistributed(layers.Dense(MAX_NUM_WORDS, activation='softmax'))\n",
    "# apply dense to output state of every timestep\n",
    "# print (decoder_outputs.shape)\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([all_section_input, decoder_input], decoder_outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "encoder_model = Model(all_section_input, encoder_states)\n",
    "\n",
    "# we also need to train a decoder model, used for inference\n",
    "# the input for the decoder model is not only the summary, but also the initial context vector\n",
    "# in inference this is the states from the encoder, used at the inital vector for decoding\n",
    "decoder_state_inputs = [layers.Input(shape=(HIDDEN_UNITS,)), layers.Input(shape=(HIDDEN_UNITS,))]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embed, initial_state=decoder_state_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_input]+decoder_state_inputs, [decoder_outputs]+decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, None, 500)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 64)     221298      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         multiple             200050      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 32), (None,  12416       time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, 150, 32), (N 10624       embedding_1[1][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 150, 4000)    132000      lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 376,338\n",
      "Trainable params: 376,338\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, None, 500)         0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 64)          221298    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                [(None, 32), (None, 32),  12416     \n",
      "=================================================================\n",
      "Total params: 233,714\n",
      "Trainable params: 233,714\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         multiple             200050      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, 150, 32), (N 10624       embedding_1[1][0]                \n",
      "                                                                 input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 150, 4000)    132000      lstm_3[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 342,674\n",
      "Trainable params: 342,674\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(self, weight_file_path):\n",
    "    if os.path.exists(weight_file_path):\n",
    "        model.load_weights(weight_file_path)\n",
    "\n",
    "def get_weight_path(model_dir_path):\n",
    "    return model_dir_path + '/' + MODEL_NAME + '-weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note the x_samples and y_samples are already tokenized and padded\n",
    "# in the vocab should add in a <eos> \n",
    "# decoder input: x y <eos>, decoder target: <sos> x y\n",
    "def generate_batch(x_samples, y_samples, batch_size=1):\n",
    "    num_batches = len(x_samples)//batch_size\n",
    "    while True:\n",
    "        for batchIdx in range(0, num_batches):\n",
    "            start = batchIdx * batch_size \n",
    "            end = (batchIdx + 1) * batch_size\n",
    "            encoder_input_batch = np.array([x_samples[start]])\n",
    "            print ('encoder', encoder_input_batch.shape)\n",
    "            decoder_target_batch = (y_samples[start])\n",
    "            decoder_input_batch = np.array([[SOS_IDX]+list(decoder_target_batch[:-1])])\n",
    "            print ('decoder', decoder_input_batch.shape)\n",
    "            \n",
    "            yield [encoder_input_batch, decoder_input_batch], decoder_target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can add more callback tricks like early stoppping etc\n",
    "def fit(Xtrain, Ytrain, epochs=EPOCHS, batch_size=BATCH_SIZE, model_dir_path=None):\n",
    "    if model_dir_path is None:\n",
    "        model_dir_path = \"./models\"\n",
    "    weight_file_path = get_weight_path(model_dir_path)\n",
    "    checkpoint = ModelCheckpoint(weight_file_path)\n",
    "    \n",
    "    train_gen = generate_batch(Xtrain, Ytrain, batch_size)\n",
    "    \n",
    "    train_num_batches = len(Xtrain) // batch_size\n",
    "    \n",
    "    # can't do validation like that\n",
    "    # need to implement decoder for inference\n",
    "#     history = model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches,\n",
    "#                                   epochs=epochs, verbose=1, \n",
    "#                                   validation_data=val_gen, validation_steps=val_num_batches,\n",
    "#                                   callbacks=[checkpoint])\n",
    "    history = model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches,\n",
    "                                  epochs=epochs, verbose=1, \n",
    "                                  callbacks=[checkpoint])\n",
    "    model.save_weights(weight_file_path)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2encoder (1, 15, 500)\n",
      "decoder (1, 150)\n",
      "\n",
      "encoder (1, 15, 500)\n",
      "decoder (1, 150)\n",
      "encoder (1, 15, 500)\n",
      "decoder (1, 150)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected time_distributed_2 to have 3 dimensions, but got array with shape (150, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d3077b5d2d95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-3cacb1faaa4a>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(Xtrain, Ytrain, epochs, batch_size, model_dir_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m     history = model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches,\n\u001b[1;32m     19\u001b[0m                                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                                   callbacks=[checkpoint])\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/kaggle/salt/venv/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/kaggle/salt/venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/kaggle/salt/venv/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    212\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/kaggle/salt/venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1207\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/kaggle/salt/venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/kaggle/salt/venv/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    125\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected time_distributed_2 to have 3 dimensions, but got array with shape (150, 1)"
     ]
    }
   ],
   "source": [
    "fit(X, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference decoder\n",
    "# the input is tokenized and padded validation/test data\n",
    "# input shape: (batch_size, encoder_max_len)\n",
    "def summarize(input_seq):\n",
    "    batch_size = len(input_seq)\n",
    "    input_seq_emb = np.zeros(shape=(batch_size, ENCODER_MAX_LEN, EMBEDDING_DIM))\n",
    "    for idx in input_seq"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
