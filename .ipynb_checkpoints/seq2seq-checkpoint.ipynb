{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Embedding, Input, LSTM, TimeDistributed, Dense\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_DIR = \"train_sections_new.txt\"\n",
    "SUMM_DIR = \"train_summary_new.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# new_text = \"train_sections_new.txt\"\n",
    "# with open(new_text, 'w') as f_out:\n",
    "#     with open(TEXT_DIR, 'r') as f_in:\n",
    "#         for line in f_in:\n",
    "#             line = eval(line)\n",
    "#             new_line = [' '.join(section) for section in line]\n",
    "#             new_line = ' '.join(new_line)\n",
    "#             re.sub(\"\\n\",  \" \", new_line)\n",
    "#             f_out.write(str(new_line.split())+'\\n')\n",
    "# import re\n",
    "# with open(SUMM_DIR, 'w') as f_out:\n",
    "#     with open('train_summary_processed.txt', 'r') as f_in:\n",
    "#         for line in f_in:\n",
    "#             re.sub(\"\\n\",  \" \", line)\n",
    "#             f_out.write(str(line.split())+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set hyper parameters\n",
    "MAX_NUM_WORDS = 10000 #vocab_size\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_UNITS = 32\n",
    "VAL_SPLIT = 0.1\n",
    "ENCODER_MAX_LEN = 4096 #for one entire doc\n",
    "DECODER_MAX_LEN = 200\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS = 10\n",
    "MODEL_NAME = \"seq2seq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# read dataset\n",
    "# first model: treat each section as one input for the encoder\n",
    "X = []\n",
    "with open(TEXT_DIR, 'r') as f:\n",
    "    for line in f:\n",
    "        if line != None:\n",
    "            X.append(' '.join(eval(line)))\n",
    "print (len(X))\n",
    "\n",
    "Y = []\n",
    "with open(SUMM_DIR, 'r') as f:\n",
    "    f_l = list(f)\n",
    "    for line in f_l:\n",
    "        if line != None:\n",
    "            Y.append(' '.join(eval(line)))\n",
    "print (len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2940.41\n",
      "206.12\n"
     ]
    }
   ],
   "source": [
    "print (np.mean([len(line.split()) for line in X]))\n",
    "print (np.mean([len(line.split()) for line in Y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10759\n",
      "373\n"
     ]
    }
   ],
   "source": [
    "print (max([len(line.split()) for line in X]))\n",
    "print (max([len(line.split()) for line in Y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 is reserved fot padding, 1 for <UNK>, word idx starts from 2\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=1)\n",
    "# must feed in a list of list of strings\n",
    "tokenizer.fit_on_texts(X+Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15479 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print (\"Found %s unique tokens.\" % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1,\n",
       " 'the': 2,\n",
       " 'of': 3,\n",
       " 'and': 4,\n",
       " 'in': 5,\n",
       " 'to': 6,\n",
       " 'a': 7,\n",
       " 'with': 8,\n",
       " 'for': 9,\n",
       " 'was': 10,\n",
       " 'is': 11,\n",
       " 'were': 12,\n",
       " 'that': 13,\n",
       " 'as': 14,\n",
       " 'by': 15,\n",
       " 'or': 16,\n",
       " '1': 17,\n",
       " 'on': 18,\n",
       " 'from': 19,\n",
       " 'be': 20,\n",
       " 'are': 21,\n",
       " 'this': 22,\n",
       " '0': 23,\n",
       " 'patients': 24,\n",
       " '2': 25,\n",
       " 'at': 26,\n",
       " 'study': 27,\n",
       " 'not': 28,\n",
       " 'an': 29,\n",
       " '3': 30,\n",
       " 'have': 31,\n",
       " '5': 32,\n",
       " 'which': 33,\n",
       " 'after': 34,\n",
       " 'between': 35,\n",
       " 'also': 36,\n",
       " 'p': 37,\n",
       " 'group': 38,\n",
       " 'may': 39,\n",
       " 'these': 40,\n",
       " 'we': 41,\n",
       " 'it': 42,\n",
       " '4': 43,\n",
       " 'mice': 44,\n",
       " '6': 45,\n",
       " 'been': 46,\n",
       " 'high': 47,\n",
       " 'than': 48,\n",
       " 'using': 49,\n",
       " 'can': 50,\n",
       " 'has': 51,\n",
       " 'all': 52,\n",
       " 'other': 53,\n",
       " 'cells': 54,\n",
       " 'more': 55,\n",
       " 'their': 56,\n",
       " 'had': 57,\n",
       " 'used': 58,\n",
       " 'one': 59,\n",
       " '8': 60,\n",
       " 'levels': 61,\n",
       " 'significant': 62,\n",
       " 'there': 63,\n",
       " 'but': 64,\n",
       " 'who': 65,\n",
       " 'data': 66,\n",
       " 'no': 67,\n",
       " 'such': 68,\n",
       " 'our': 69,\n",
       " 'treatment': 70,\n",
       " 'b': 71,\n",
       " '7': 72,\n",
       " 'studies': 73,\n",
       " 'reported': 74,\n",
       " 'results': 75,\n",
       " 'health': 76,\n",
       " '10': 77,\n",
       " 'compared': 78,\n",
       " 'muscle': 79,\n",
       " 'age': 80,\n",
       " 'fat': 81,\n",
       " 'however': 82,\n",
       " 'associated': 83,\n",
       " 'expression': 84,\n",
       " 'type': 85,\n",
       " 'two': 86,\n",
       " 'low': 87,\n",
       " 'both': 88,\n",
       " 'positive': 89,\n",
       " 'insulin': 90,\n",
       " 'most': 91,\n",
       " 'et': 92,\n",
       " 'patient': 93,\n",
       " 'analysis': 94,\n",
       " 'cancer': 95,\n",
       " 'test': 96,\n",
       " 'during': 97,\n",
       " 'risk': 98,\n",
       " 'al': 99,\n",
       " 'figure': 100,\n",
       " 'table': 101,\n",
       " '9': 102,\n",
       " 'among': 103,\n",
       " 'g': 104,\n",
       " 'disease': 105,\n",
       " 'm': 106,\n",
       " 'showed': 107,\n",
       " '12': 108,\n",
       " 'cell': 109,\n",
       " 'based': 110,\n",
       " 'observed': 111,\n",
       " 'protein': 112,\n",
       " 'increased': 113,\n",
       " 'years': 114,\n",
       " 'control': 115,\n",
       " 'due': 116,\n",
       " 'only': 117,\n",
       " 'level': 118,\n",
       " 'found': 119,\n",
       " 'different': 120,\n",
       " 'its': 121,\n",
       " 'into': 122,\n",
       " 'time': 123,\n",
       " 'significantly': 124,\n",
       " 'blood': 125,\n",
       " 'when': 126,\n",
       " 'fig': 127,\n",
       " 'clinical': 128,\n",
       " 'performed': 129,\n",
       " 'diet': 130,\n",
       " 'each': 131,\n",
       " 'number': 132,\n",
       " 'treated': 133,\n",
       " 'c': 134,\n",
       " 'activity': 135,\n",
       " \"'s\": 136,\n",
       " 'women': 137,\n",
       " 'present': 138,\n",
       " 'body': 139,\n",
       " 'groups': 140,\n",
       " 'higher': 141,\n",
       " 'primary': 142,\n",
       " 'up': 143,\n",
       " 'including': 144,\n",
       " 'mg': 145,\n",
       " 'd': 146,\n",
       " 'weight': 147,\n",
       " 'first': 148,\n",
       " 'those': 149,\n",
       " 'although': 150,\n",
       " 'while': 151,\n",
       " '15': 152,\n",
       " 'could': 153,\n",
       " 'e': 154,\n",
       " 'aged': 155,\n",
       " 'middle': 156,\n",
       " 'use': 157,\n",
       " 'factors': 158,\n",
       " 'l': 159,\n",
       " 'well': 160,\n",
       " 'long': 161,\n",
       " 'if': 162,\n",
       " 'before': 163,\n",
       " 'because': 164,\n",
       " 'anti': 165,\n",
       " 'they': 166,\n",
       " 'shown': 167,\n",
       " 'tumor': 168,\n",
       " 'care': 169,\n",
       " 'effects': 170,\n",
       " 'common': 171,\n",
       " 'year': 172,\n",
       " 'cases': 173,\n",
       " 'glucose': 174,\n",
       " 'increase': 175,\n",
       " 'rate': 176,\n",
       " 'population': 177,\n",
       " 'total': 178,\n",
       " 'effect': 179,\n",
       " 'mean': 180,\n",
       " 'case': 181,\n",
       " 'three': 182,\n",
       " 'n': 183,\n",
       " 'acid': 184,\n",
       " 'development': 185,\n",
       " 'students': 186,\n",
       " 'some': 187,\n",
       " 'diabetes': 188,\n",
       " 'prevalence': 189,\n",
       " 'related': 190,\n",
       " 'specific': 191,\n",
       " 'obtained': 192,\n",
       " '30': 193,\n",
       " 'resistance': 194,\n",
       " 'lower': 195,\n",
       " 'obesity': 196,\n",
       " 'education': 197,\n",
       " 'difference': 198,\n",
       " 'over': 199,\n",
       " 'induced': 200,\n",
       " 'weeks': 201,\n",
       " 'gene': 202,\n",
       " 'through': 203,\n",
       " 'skeletal': 204,\n",
       " 'fed': 205,\n",
       " 'association': 206,\n",
       " '20': 207,\n",
       " 'liver': 208,\n",
       " 'follow': 209,\n",
       " 'any': 210,\n",
       " 'factor': 211,\n",
       " 'further': 212,\n",
       " 's': 213,\n",
       " 'role': 214,\n",
       " 'will': 215,\n",
       " 'serum': 216,\n",
       " 'did': 217,\n",
       " 'within': 218,\n",
       " 'considered': 219,\n",
       " 'survival': 220,\n",
       " 'surgery': 221,\n",
       " 'same': 222,\n",
       " 'thus': 223,\n",
       " 'human': 224,\n",
       " 'mental': 225,\n",
       " 'children': 226,\n",
       " 'following': 227,\n",
       " 'therapy': 228,\n",
       " 'milrinone': 229,\n",
       " 'free': 230,\n",
       " 'bmi': 231,\n",
       " '25': 232,\n",
       " 'period': 233,\n",
       " 'about': 234,\n",
       " 'seizure': 235,\n",
       " 'h': 236,\n",
       " 'therefore': 237,\n",
       " 'several': 238,\n",
       " 'demonstrated': 239,\n",
       " 'decreased': 240,\n",
       " '14': 241,\n",
       " 'normal': 242,\n",
       " '95': 243,\n",
       " 'should': 244,\n",
       " 'differences': 245,\n",
       " 'stage': 246,\n",
       " 'respectively': 247,\n",
       " '16': 248,\n",
       " 'important': 249,\n",
       " '11': 250,\n",
       " 'subjects': 251,\n",
       " 'without': 252,\n",
       " 'ml': 253,\n",
       " 'result': 254,\n",
       " 'm1dg': 255,\n",
       " 'mutation': 256,\n",
       " 'myriocin': 257,\n",
       " '100': 258,\n",
       " 'db': 259,\n",
       " 'information': 260,\n",
       " 'system': 261,\n",
       " 'presence': 262,\n",
       " 'cardiac': 263,\n",
       " '50': 264,\n",
       " 'inflammatory': 265,\n",
       " 'process': 266,\n",
       " 'included': 267,\n",
       " 'catenin': 268,\n",
       " 'determined': 269,\n",
       " 'months': 270,\n",
       " 'negative': 271,\n",
       " 'previous': 272,\n",
       " 'then': 273,\n",
       " 'like': 274,\n",
       " 'described': 275,\n",
       " 'tissue': 276,\n",
       " 'multiple': 277,\n",
       " 'diseases': 278,\n",
       " 'findings': 279,\n",
       " 'intervention': 280,\n",
       " 'similar': 281,\n",
       " 'addition': 282,\n",
       " 'research': 283,\n",
       " '24': 284,\n",
       " '18': 285,\n",
       " 'changes': 286,\n",
       " 'bone': 287,\n",
       " 'detected': 288,\n",
       " 'samples': 289,\n",
       " 'day': 290,\n",
       " 'many': 291,\n",
       " 'days': 292,\n",
       " 'species': 293,\n",
       " '13': 294,\n",
       " 'i': 295,\n",
       " 'under': 296,\n",
       " 'dna': 297,\n",
       " 'out': 298,\n",
       " 'variables': 299,\n",
       " 'ratio': 300,\n",
       " 'function': 301,\n",
       " 'status': 302,\n",
       " 'reduction': 303,\n",
       " 'either': 304,\n",
       " 'diagnosis': 305,\n",
       " 'via': 306,\n",
       " 'oxo': 307,\n",
       " 'dsi': 308,\n",
       " 'revealed': 309,\n",
       " 'known': 310,\n",
       " 'incidence': 311,\n",
       " 'formation': 312,\n",
       " '05': 313,\n",
       " 'four': 314,\n",
       " 'term': 315,\n",
       " 'since': 316,\n",
       " 'animals': 317,\n",
       " 'non': 318,\n",
       " 'kg': 319,\n",
       " 'potential': 320,\n",
       " 'ct': 321,\n",
       " 'young': 322,\n",
       " 'mm': 323,\n",
       " 'conducted': 324,\n",
       " 'model': 325,\n",
       " '21': 326,\n",
       " 'identified': 327,\n",
       " 't': 328,\n",
       " 'ci': 329,\n",
       " 'physical': 330,\n",
       " 'sample': 331,\n",
       " 'range': 332,\n",
       " 'less': 333,\n",
       " 'rates': 334,\n",
       " 'furthermore': 335,\n",
       " 'being': 336,\n",
       " 'followed': 337,\n",
       " 'large': 338,\n",
       " 'early': 339,\n",
       " 'loss': 340,\n",
       " 'individuals': 341,\n",
       " 'spread': 342,\n",
       " 'new': 343,\n",
       " 'reduced': 344,\n",
       " 'molecular': 345,\n",
       " 'antibody': 346,\n",
       " 'showing': 347,\n",
       " 'apoe': 348,\n",
       " 'symptoms': 349,\n",
       " 'overall': 350,\n",
       " 'activation': 351,\n",
       " 'previously': 352,\n",
       " 'lipid': 353,\n",
       " 'sensitivity': 354,\n",
       " 'depression': 355,\n",
       " 'might': 356,\n",
       " 'determine': 357,\n",
       " 'participants': 358,\n",
       " 'evidence': 359,\n",
       " 'inflammation': 360,\n",
       " 'response': 361,\n",
       " 'least': 362,\n",
       " 'likely': 363,\n",
       " 'antibodies': 364,\n",
       " 'immune': 365,\n",
       " 'contrast': 366,\n",
       " 'direct': 367,\n",
       " 'according': 368,\n",
       " 'report': 369,\n",
       " 'decrease': 370,\n",
       " 'r': 371,\n",
       " 'rad6': 372,\n",
       " 'measured': 373,\n",
       " 'frequency': 374,\n",
       " 'production': 375,\n",
       " 'possible': 376,\n",
       " 'gcc': 377,\n",
       " 'analyzed': 378,\n",
       " '19': 379,\n",
       " 'seen': 380,\n",
       " 'products': 381,\n",
       " 'methods': 382,\n",
       " 'gas6': 383,\n",
       " 'area': 384,\n",
       " 'mass': 385,\n",
       " 'method': 386,\n",
       " '17': 387,\n",
       " 'dl': 388,\n",
       " 'lesions': 389,\n",
       " 'oxidation': 390,\n",
       " 'ana': 391,\n",
       " 'usa': 392,\n",
       " 'cause': 393,\n",
       " 'given': 394,\n",
       " 'small': 395,\n",
       " 'hospital': 396,\n",
       " 'iron': 397,\n",
       " '001': 398,\n",
       " 'whole': 399,\n",
       " 'history': 400,\n",
       " 'receptor': 401,\n",
       " 'adults': 402,\n",
       " 'standard': 403,\n",
       " 'knowledge': 404,\n",
       " 'effective': 405,\n",
       " 'secondary': 406,\n",
       " 'assessment': 407,\n",
       " '60': 408,\n",
       " 'right': 409,\n",
       " 'metastatic': 410,\n",
       " 'exercise': 411,\n",
       " 'cd36': 412,\n",
       " 'second': 413,\n",
       " 'developed': 414,\n",
       " 'tumour': 415,\n",
       " 'better': 416,\n",
       " 'order': 417,\n",
       " 'form': 418,\n",
       " 'infection': 419,\n",
       " 'single': 420,\n",
       " 'mir': 421,\n",
       " 'artery': 422,\n",
       " 'il': 423,\n",
       " 'breast': 424,\n",
       " 'nodes': 425,\n",
       " 'hs': 426,\n",
       " 'chromophore': 427,\n",
       " 'wcb': 428,\n",
       " 'medical': 429,\n",
       " 'adolescents': 430,\n",
       " 'especially': 431,\n",
       " 'renal': 432,\n",
       " 'chain': 433,\n",
       " 'obese': 434,\n",
       " 'dsp': 435,\n",
       " 'solution': 436,\n",
       " 'gastrocnemius': 437,\n",
       " 'value': 438,\n",
       " 'even': 439,\n",
       " 'cm': 440,\n",
       " 'another': 441,\n",
       " 'proteins': 442,\n",
       " 'sites': 443,\n",
       " 'testing': 444,\n",
       " 'recent': 445,\n",
       " 'values': 446,\n",
       " 'post': 447,\n",
       " 'binding': 448,\n",
       " '23': 449,\n",
       " 'genes': 450,\n",
       " 'size': 451,\n",
       " 'involved': 452,\n",
       " 'ko': 453,\n",
       " 'assay': 454,\n",
       " '40': 455,\n",
       " 'against': 456,\n",
       " 'erg': 457,\n",
       " 'pcos': 458,\n",
       " 'food': 459,\n",
       " 'product': 460,\n",
       " 'general': 461,\n",
       " 'involvement': 462,\n",
       " 'min': 463,\n",
       " 'ad': 464,\n",
       " 'crp': 465,\n",
       " 'them': 466,\n",
       " 'above': 467,\n",
       " 'current': 468,\n",
       " 'tumors': 469,\n",
       " 'rats': 470,\n",
       " 'growth': 471,\n",
       " 'tests': 472,\n",
       " 'independent': 473,\n",
       " 'provided': 474,\n",
       " 'five': 475,\n",
       " 'recently': 476,\n",
       " 'pathway': 477,\n",
       " 'moreover': 478,\n",
       " 'shows': 479,\n",
       " 'left': 480,\n",
       " 'signaling': 481,\n",
       " 'bacteria': 482,\n",
       " 'design': 483,\n",
       " 'counseling': 484,\n",
       " 'ceramide': 485,\n",
       " 'average': 486,\n",
       " 'boys': 487,\n",
       " 'received': 488,\n",
       " 'exposure': 489,\n",
       " 'conditions': 490,\n",
       " 'vein': 491,\n",
       " 'include': 492,\n",
       " 'show': 493,\n",
       " 'characteristics': 494,\n",
       " 'comparison': 495,\n",
       " '26': 496,\n",
       " 'srcc': 497,\n",
       " 'expressed': 498,\n",
       " 'lung': 499,\n",
       " 'mediated': 500,\n",
       " 'diagnosed': 501,\n",
       " 'heart': 502,\n",
       " 'mutations': 503,\n",
       " 'fatty': 504,\n",
       " 'nutrition': 505,\n",
       " 'university': 506,\n",
       " 'male': 507,\n",
       " 'major': 508,\n",
       " 'per': 509,\n",
       " 'presented': 510,\n",
       " 'dependent': 511,\n",
       " 'where': 512,\n",
       " 'plasma': 513,\n",
       " 'pcr': 514,\n",
       " 'rif': 515,\n",
       " 'examined': 516,\n",
       " 'tolerance': 517,\n",
       " 'inhibition': 518,\n",
       " 'detection': 519,\n",
       " 'having': 520,\n",
       " 'wild': 521,\n",
       " 'relationship': 522,\n",
       " 'ms': 523,\n",
       " 'would': 524,\n",
       " 'chronic': 525,\n",
       " 'site': 526,\n",
       " 'mouse': 527,\n",
       " 'vascular': 528,\n",
       " 'metabolic': 529,\n",
       " 'local': 530,\n",
       " 'main': 531,\n",
       " 'state': 532,\n",
       " '27': 533,\n",
       " 'activated': 534,\n",
       " 'melanoma': 535,\n",
       " 'people': 536,\n",
       " 'dm': 537,\n",
       " 'prevention': 538,\n",
       " 'sd': 539,\n",
       " 'score': 540,\n",
       " 'statistically': 541,\n",
       " 'suggest': 542,\n",
       " '35': 543,\n",
       " 'events': 544,\n",
       " 'injection': 545,\n",
       " 'whether': 546,\n",
       " 'disorders': 547,\n",
       " 'review': 548,\n",
       " 'life': 549,\n",
       " 'end': 550,\n",
       " 'female': 551,\n",
       " 'target': 552,\n",
       " 'laboratory': 553,\n",
       " '22': 554,\n",
       " '28': 555,\n",
       " 'distribution': 556,\n",
       " 'dg': 557,\n",
       " 'very': 558,\n",
       " 'intake': 559,\n",
       " 'week': 560,\n",
       " 'vitamin': 561,\n",
       " 'available': 562,\n",
       " 'outcome': 563,\n",
       " 'reaction': 564,\n",
       " 'versus': 565,\n",
       " 'pain': 566,\n",
       " 'correlation': 567,\n",
       " 'root': 568,\n",
       " 'community': 569,\n",
       " 'old': 570,\n",
       " 'imaging': 571,\n",
       " 'cholesterol': 572,\n",
       " 'mri': 573,\n",
       " 'consumption': 574,\n",
       " 'arrows': 575,\n",
       " 'tnet': 576,\n",
       " 'girls': 577,\n",
       " 'family': 578,\n",
       " 'types': 579,\n",
       " 'often': 580,\n",
       " 'nf': 581,\n",
       " 'anterior': 582,\n",
       " 'selected': 583,\n",
       " 'pattern': 584,\n",
       " 'receptors': 585,\n",
       " 'concentration': 586,\n",
       " 'f': 587,\n",
       " 'water': 588,\n",
       " 'see': 589,\n",
       " 'anxiety': 590,\n",
       " 'hiv': 591,\n",
       " 'dox': 592,\n",
       " 'kimchi': 593,\n",
       " 'tb': 594,\n",
       " 'energy': 595,\n",
       " '29': 596,\n",
       " 'improved': 597,\n",
       " 'elevated': 598,\n",
       " 'median': 599,\n",
       " 'infections': 600,\n",
       " 'structures': 601,\n",
       " 'identify': 602,\n",
       " 'apheresis': 603,\n",
       " 'collected': 604,\n",
       " 'nm': 605,\n",
       " 'der': 606,\n",
       " 'ppar': 607,\n",
       " 'hdt': 608,\n",
       " 'mtex101': 609,\n",
       " 'program': 610,\n",
       " 'induction': 611,\n",
       " 'retinal': 612,\n",
       " 'freedom': 613,\n",
       " 'cect': 614,\n",
       " 'reports': 615,\n",
       " 'chemotherapy': 616,\n",
       " 'functioning': 617,\n",
       " '42': 618,\n",
       " 'various': 619,\n",
       " 'internal': 620,\n",
       " 'solutions': 621,\n",
       " 'bse': 622,\n",
       " 'peripheral': 623,\n",
       " 'oxygen': 624,\n",
       " 'cytokines': 625,\n",
       " 'ks': 626,\n",
       " 'pulp': 627,\n",
       " 'areas': 628,\n",
       " 'scores': 629,\n",
       " 'short': 630,\n",
       " 'progression': 631,\n",
       " 'greater': 632,\n",
       " 'tissues': 633,\n",
       " 'membrane': 634,\n",
       " 'cognitive': 635,\n",
       " 'so': 636,\n",
       " 'analyses': 637,\n",
       " '70': 638,\n",
       " 'do': 639,\n",
       " 'complete': 640,\n",
       " 'highest': 641,\n",
       " 'suggested': 642,\n",
       " 'part': 643,\n",
       " 'x': 644,\n",
       " 'amp': 645,\n",
       " 'controls': 646,\n",
       " 'acute': 647,\n",
       " 'coa': 648,\n",
       " 'containing': 649,\n",
       " 'occur': 650,\n",
       " 'additional': 651,\n",
       " 'mitochondrial': 652,\n",
       " 'head': 653,\n",
       " 'social': 654,\n",
       " 'increases': 655,\n",
       " 'oral': 656,\n",
       " 'men': 657,\n",
       " 'mechanism': 658,\n",
       " 'regression': 659,\n",
       " 'prior': 660,\n",
       " 'self': 661,\n",
       " 'specimens': 662,\n",
       " 'condition': 663,\n",
       " 'metabolism': 664,\n",
       " 'along': 665,\n",
       " 'region': 666,\n",
       " 'severe': 667,\n",
       " '31': 668,\n",
       " 'pathways': 669,\n",
       " 'brain': 670,\n",
       " 'elderly': 671,\n",
       " 'suggesting': 672,\n",
       " 'edema': 673,\n",
       " 'specificity': 674,\n",
       " 'malignant': 675,\n",
       " 'metastasis': 676,\n",
       " 'mau': 677,\n",
       " 'fractures': 678,\n",
       " 'phosphorylation': 679,\n",
       " 'dentin': 680,\n",
       " '80': 681,\n",
       " 'examination': 682,\n",
       " 'done': 683,\n",
       " 'side': 684,\n",
       " 'systems': 685,\n",
       " 'onset': 686,\n",
       " 'combined': 687,\n",
       " 'complications': 688,\n",
       " 'ph': 689,\n",
       " 'amyloid': 690,\n",
       " 'approximately': 691,\n",
       " 'invasion': 692,\n",
       " 'pmn': 693,\n",
       " 'developing': 694,\n",
       " 'lack': 695,\n",
       " 'statistical': 696,\n",
       " 'duration': 697,\n",
       " 'screening': 698,\n",
       " 'way': 699,\n",
       " 'fatigue': 700,\n",
       " 'criteria': 701,\n",
       " 'mechanisms': 702,\n",
       " 'components': 703,\n",
       " 'initial': 704,\n",
       " 'resistant': 705,\n",
       " 'light': 706,\n",
       " 'metastases': 707,\n",
       " 'w': 708,\n",
       " 'headache': 709,\n",
       " 'enhanced': 710,\n",
       " 'active': 711,\n",
       " 'leading': 712,\n",
       " 'required': 713,\n",
       " 'example': 714,\n",
       " 'yield': 715,\n",
       " 'genetic': 716,\n",
       " '38': 717,\n",
       " 'caused': 718,\n",
       " 'syndrome': 719,\n",
       " 'ii': 720,\n",
       " 'venous': 721,\n",
       " 'acyl': 722,\n",
       " 'vs': 723,\n",
       " 'endothelial': 724,\n",
       " 'best': 725,\n",
       " 'center': 726,\n",
       " 'healthy': 727,\n",
       " 'measures': 728,\n",
       " 'capacity': 729,\n",
       " 'combination': 730,\n",
       " 'cycle': 731,\n",
       " 'last': 732,\n",
       " 'confirmed': 733,\n",
       " 'stress': 734,\n",
       " 'lifestyle': 735,\n",
       " 'spinal': 736,\n",
       " 'still': 737,\n",
       " 'canal': 738,\n",
       " 'suicide': 739,\n",
       " 'gfp': 740,\n",
       " 'igg4': 741,\n",
       " 'school': 742,\n",
       " 'poor': 743,\n",
       " 'baseline': 744,\n",
       " 'confidence': 745,\n",
       " '90': 746,\n",
       " 'functional': 747,\n",
       " 'lead': 748,\n",
       " 'toll': 749,\n",
       " 'consistent': 750,\n",
       " 'hours': 751,\n",
       " 'investigated': 752,\n",
       " 'individual': 753,\n",
       " 'ethanol': 754,\n",
       " 'falls': 755,\n",
       " 'prostate': 756,\n",
       " 'national': 757,\n",
       " 'regarding': 758,\n",
       " 'hand': 759,\n",
       " 'questionnaire': 760,\n",
       " 'reduce': 761,\n",
       " 'defects': 762,\n",
       " 'does': 763,\n",
       " 'management': 764,\n",
       " 'need': 765,\n",
       " '36': 766,\n",
       " 'relatively': 767,\n",
       " '210': 768,\n",
       " 'times': 769,\n",
       " 'directly': 770,\n",
       " 'derived': 771,\n",
       " 'nerve': 772,\n",
       " 'ankle': 773,\n",
       " '48': 774,\n",
       " 'nevi': 775,\n",
       " 'provide': 776,\n",
       " 'survey': 777,\n",
       " 'degree': 778,\n",
       " 'assessed': 779,\n",
       " 'change': 780,\n",
       " 'parameters': 781,\n",
       " 'increasing': 782,\n",
       " 'characterized': 783,\n",
       " 'he': 784,\n",
       " 'complex': 785,\n",
       " 'nuclear': 786,\n",
       " 'signal': 787,\n",
       " '54': 788,\n",
       " 'functions': 789,\n",
       " 'pressure': 790,\n",
       " 'indicated': 791,\n",
       " 'aging': 792,\n",
       " 'able': 793,\n",
       " 'structure': 794,\n",
       " 'sequence': 795,\n",
       " 'stroke': 796,\n",
       " 'crispr': 797,\n",
       " 'support': 798,\n",
       " 'activities': 799,\n",
       " 'literature': 800,\n",
       " 'trials': 801,\n",
       " 'studied': 802,\n",
       " 'death': 803,\n",
       " 'central': 804,\n",
       " 'work': 805,\n",
       " 'genome': 806,\n",
       " 'few': 807,\n",
       " 'play': 808,\n",
       " 'procedure': 809,\n",
       " 'allele': 810,\n",
       " 'set': 811,\n",
       " 'older': 812,\n",
       " 'memory': 813,\n",
       " 'content': 814,\n",
       " 'whereas': 815,\n",
       " 'antigens': 816,\n",
       " 'cas9': 817,\n",
       " 'icf': 818,\n",
       " 'nodal': 819,\n",
       " 'dio': 820,\n",
       " 'carried': 821,\n",
       " 'interventions': 822,\n",
       " 'wall': 823,\n",
       " 'established': 824,\n",
       " 'line': 825,\n",
       " 'point': 826,\n",
       " 'defined': 827,\n",
       " 'delivery': 828,\n",
       " 'tested': 829,\n",
       " 'sustained': 830,\n",
       " 'disability': 831,\n",
       " 'led': 832,\n",
       " 'staining': 833,\n",
       " 'infusion': 834,\n",
       " 'defect': 835,\n",
       " '37': 836,\n",
       " '34': 837,\n",
       " 'amount': 838,\n",
       " 'esi': 839,\n",
       " 'calculated': 840,\n",
       " 'evaluation': 841,\n",
       " '44': 842,\n",
       " 'reactions': 843,\n",
       " 'dose': 844,\n",
       " 'systemic': 845,\n",
       " '75': 846,\n",
       " 'adult': 847,\n",
       " 'cellular': 848,\n",
       " 'highly': 849,\n",
       " 'usually': 850,\n",
       " 'epilepsy': 851,\n",
       " 'respiratory': 852,\n",
       " 'joint': 853,\n",
       " 'diagnostic': 854,\n",
       " 'rather': 855,\n",
       " 'tertiary': 856,\n",
       " 'tnf': 857,\n",
       " \"'\": 858,\n",
       " 'mtb': 859,\n",
       " 'axial': 860,\n",
       " 'off': 861,\n",
       " 'z': 862,\n",
       " 'needed': 863,\n",
       " 'adverse': 864,\n",
       " 'despite': 865,\n",
       " 'impaired': 866,\n",
       " 'every': 867,\n",
       " 'inappropriate': 868,\n",
       " 'particular': 869,\n",
       " 'transcription': 870,\n",
       " 'relative': 871,\n",
       " 'infertility': 872,\n",
       " 'igf2': 873,\n",
       " 'resulting': 874,\n",
       " 'kidney': 875,\n",
       " 'visual': 876,\n",
       " 'cox': 877,\n",
       " 'buffer': 878,\n",
       " '68': 879,\n",
       " 'isl1': 880,\n",
       " 'technique': 881,\n",
       " 'net': 882,\n",
       " 'dairy': 883,\n",
       " 'tumours': 884,\n",
       " 'approach': 885,\n",
       " 'populations': 886,\n",
       " 'made': 887,\n",
       " 'responses': 888,\n",
       " 'particularly': 889,\n",
       " 'volume': 890,\n",
       " 'imd': 891,\n",
       " '32': 892,\n",
       " 'ca': 893,\n",
       " 'key': 894,\n",
       " 'posterior': 895,\n",
       " 'lateral': 896,\n",
       " 'commonly': 897,\n",
       " 'oxidative': 898,\n",
       " 'vivo': 899,\n",
       " 'cross': 900,\n",
       " 'phase': 901,\n",
       " 'evaluated': 902,\n",
       " 'genotype': 903,\n",
       " 'invasive': 904,\n",
       " '33': 905,\n",
       " 'estimated': 906,\n",
       " '41': 907,\n",
       " 'stained': 908,\n",
       " 'prevent': 909,\n",
       " 'frequently': 910,\n",
       " 'operated': 911,\n",
       " 'vertebral': 912,\n",
       " 'proinflammatory': 913,\n",
       " 'scale': 914,\n",
       " 'spss': 915,\n",
       " 'sex': 916,\n",
       " 'impact': 917,\n",
       " 'fact': 918,\n",
       " 'added': 919,\n",
       " 'appropriate': 920,\n",
       " 'lepidoptera': 921,\n",
       " 'currently': 922,\n",
       " 'applied': 923,\n",
       " 'surgical': 924,\n",
       " 'outcomes': 925,\n",
       " 'ovarian': 926,\n",
       " 'stimulated': 927,\n",
       " 'deposits': 928,\n",
       " 'vehicle': 929,\n",
       " 'mesenteric': 930,\n",
       " 'nursing': 931,\n",
       " '3243a': 932,\n",
       " 'index': 933,\n",
       " 'performance': 934,\n",
       " 'consent': 935,\n",
       " 'cost': 936,\n",
       " 'inhibitor': 937,\n",
       " '47': 938,\n",
       " '55': 939,\n",
       " 'intracellular': 940,\n",
       " 'kinase': 941,\n",
       " 'proximal': 942,\n",
       " 'processes': 943,\n",
       " 'month': 944,\n",
       " 'square': 945,\n",
       " 'a142': 946,\n",
       " 'classification': 947,\n",
       " 'accumulation': 948,\n",
       " 'field': 949,\n",
       " 'adducts': 950,\n",
       " 'exposed': 951,\n",
       " 't2d': 952,\n",
       " 'injections': 953,\n",
       " 'krg': 954,\n",
       " 'anova': 955,\n",
       " 'asterisk': 956,\n",
       " 'must': 957,\n",
       " 'agents': 958,\n",
       " 'course': 959,\n",
       " '000': 960,\n",
       " 'plant': 961,\n",
       " 'animal': 962,\n",
       " 'purified': 963,\n",
       " 'detect': 964,\n",
       " 'resection': 965,\n",
       " 'ldl': 966,\n",
       " 'coronary': 967,\n",
       " 'isolated': 968,\n",
       " 'final': 969,\n",
       " 'melanomas': 970,\n",
       " 'feeding': 971,\n",
       " 'improvement': 972,\n",
       " '56': 973,\n",
       " 'carcinoma': 974,\n",
       " '45': 975,\n",
       " 'therapeutic': 976,\n",
       " 'western': 977,\n",
       " '64': 978,\n",
       " 'core': 979,\n",
       " 'pesticide': 980,\n",
       " 'fracture': 981,\n",
       " 'soft': 982,\n",
       " 'sway': 983,\n",
       " 'hba1c': 984,\n",
       " 'abo': 985,\n",
       " 'height': 986,\n",
       " '01': 987,\n",
       " 'features': 988,\n",
       " 'optimal': 989,\n",
       " 'melanogaster': 990,\n",
       " 'identification': 991,\n",
       " 'apoptosis': 992,\n",
       " 'experiments': 993,\n",
       " 'finally': 994,\n",
       " 'affected': 995,\n",
       " 'injury': 996,\n",
       " 'medications': 997,\n",
       " 'absence': 998,\n",
       " 'procedures': 999,\n",
       " 'mn': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index\n",
    "# index 0: for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add <sos>, <eos> token, <UNK>(OOV) is idx 0 by default\n",
    "vocab_word_index = {}\n",
    "vocab_index_word = {}\n",
    "for word, idx in word_index.items():\n",
    "    if idx < MAX_NUM_WORDS - 2:\n",
    "        vocab_word_index[word] = idx\n",
    "        vocab_index_word[idx] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_word_index['<sos>'] = MAX_NUM_WORDS - 1\n",
    "vocab_index_word[MAX_NUM_WORDS - 1] = '<sos>'\n",
    "vocab_word_index['<eos>'] = MAX_NUM_WORDS - 2\n",
    "vocab_index_word[MAX_NUM_WORDS - 2] = '<eos>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_index_word[0] = ' '\n",
    "vocab_index_word[1] = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_IDX = MAX_NUM_WORDS - 2\n",
    "SOS_IDX = MAX_NUM_WORDS - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert words to indices\n",
    "#Note: here we pad each section and use section for encoder\n",
    "#Another alternative is to use sentence as encoder and pad each sentence\n",
    "# note: encoder does not need <sos> or <eos>\n",
    "# for encoder input, pad in front\n",
    "# decoder input, pad in the end\n",
    "# note that number of sections is not constant for every article\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "X = pad_sequences(X, ENCODER_MAX_LEN, padding='pre')\n",
    "    \n",
    "Y = tokenizer.texts_to_sequences(Y)\n",
    "for line in Y:\n",
    "    line.append(EOS_IDX)\n",
    "Y = pad_sequences(Y, DECODER_MAX_LEN, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 21,  36, 187, ..., 742, 155, 226], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  13,  372,    4,  268, 2695,  131,   53,  136,   84,  203,    7,\n",
       "         89, 5050, 5699,    6, 1792,  424,   95,  185,  631,  151,  268,\n",
       "         51,   46, 1740,    5,  535, 1236,  372,  301,   51,   28,   46,\n",
       "        752, 1198,   41,  516,    2,  522,   35,  372,    4,  268,    5,\n",
       "        535,  185,    4,  631, 3536, 1660, 2883,  469,  193,  775,  596,\n",
       "        142,  535,    4,  596,  410,  970,   12, 6657,    8,  165,  268,\n",
       "          4,  165,  372,  364, 1197,   84,    3,  372,   10,  111,    5,\n",
       "        117,  533,    3,  775,   14,   78,    6,  258,    3,  142,    4,\n",
       "       1370,    3,  410,  970,  268,   10, 1726,  498,    5, 1667,    3,\n",
       "        142,    4, 1420,    3,  410,  970,    4, 3141,  372,    5, 1420,\n",
       "          3,  775, 1291,    3,    2,  469,  498,  786,  268,  268,   10,\n",
       "       3348, 1455,   18,    2,  109,  634,    3,  939,    3,  142, 2126,\n",
       "          3,  410,  970,    4,  117,   77,    3,  775, 1811,  268,   10,\n",
       "        288,    5,  746,    3,  775,  387,    3,  142,    4,   60,    3,\n",
       "        410,  535,  815,  555,    3,  142,    4,  193,    3,  410,  970,\n",
       "       1632,  268,   26,   88, 5006,   40,   66,  542,   13,  535,  185,\n",
       "          4,  631,   21,   83,    8,  372, 2124,    4, 2881, 6664,    3,\n",
       "        268,    4,   13,  268,    4,  372,  808,  473, 1454,    5,  535,\n",
       "        185, 9998], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_samples = len(X)\n",
    "# x_train = X[:-int(n_samples*VAL_SPLIT)]\n",
    "# y_train = Y[:-int(n_samples*VAL_SPLIT)]\n",
    "\n",
    "# x_val = X[-int(n_samples*VAL_SPLIT):]\n",
    "# y_val = Y[-int(n_samples*VAL_SPLIT):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4096)\n",
      "(100, 200)\n"
     ]
    }
   ],
   "source": [
    "print (X.shape)\n",
    "print (Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use pretrained 50d glove vectors\n",
    "#with larger dataset, we can try glove woith higher dimension or \n",
    "#learn word embedding from scratch\n",
    "# vector for UNK and those not in glove are randomly initialized\n",
    "GLOVE_DIR = \"../glove.6B.50d.txt\"\n",
    "f_emb = open(GLOVE_DIR, 'r')\n",
    "embedding_index = {}\n",
    "for line in f_emb:\n",
    "    line = line.strip().split()\n",
    "    word = line[0]\n",
    "    coefs = np.asarray(line[1:], dtype='float32')\n",
    "    embedding_index[word] = coefs\n",
    "f_emb.close()\n",
    "\n",
    "embedding_matrix = np.random.random((MAX_NUM_WORDS+1, EMBEDDING_DIM))\n",
    "for word, i in vocab_word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slice sequences into many subsequences\n",
    "X_split=[]\n",
    "for i in range(X.shape[0]):\n",
    "    split1=np.split(X[i],8)\n",
    "    a=[]\n",
    "    for j in range(8):\n",
    "        s=np.split(split1[j],8)\n",
    "        a.append(s)\n",
    "    X_split.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 8, 8, 64)\n"
     ]
    }
   ],
   "source": [
    "X_split = np.array(X_split)\n",
    "print (X_split.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(MAX_NUM_WORDS + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=int(ENCODER_MAX_LEN/64),\n",
    "                            trainable=True)\n",
    "\n",
    "input1 = Input(shape=(int(ENCODER_MAX_LEN/64),), dtype='int32')\n",
    "embed = embedding_layer(input1)\n",
    "lstm1 = LSTM(HIDDEN_UNITS)(embed)\n",
    "Encoder1 = Model(input1, lstm1)\n",
    "\n",
    "input2 = Input(shape=(8,int(ENCODER_MAX_LEN/64),), dtype='int32')\n",
    "embed2 = TimeDistributed(Encoder1)(input2)\n",
    "lstm2 = LSTM(HIDDEN_UNITS)(embed2)\n",
    "Encoder2 = Model(input2,lstm2)\n",
    "\n",
    "all_input = Input(shape=(8,8,int(ENCODER_MAX_LEN/64)), dtype='int32')\n",
    "embed3 = TimeDistributed(Encoder2)(all_input)\n",
    "_, encoder_state_h, encoder_state_c = LSTM(HIDDEN_UNITS, return_state=True)(embed3)\n",
    "\n",
    "encoder_states = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "#in training this is the summary, in inference this is the previous word\n",
    "# decode input is one-hot index, not embeddings!\n",
    "# use None to allow variable decoder length, \n",
    "# because in inference we are decoding one at a time\n",
    "decoder_input = Input(shape=(None,MAX_NUM_WORDS))\n",
    "decoder_lstm = LSTM(HIDDEN_UNITS, return_state=True, return_sequences=True)\n",
    "decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_input, initial_state=encoder_states)\n",
    "decoder_dense = Dense(MAX_NUM_WORDS, activation='softmax')\n",
    "# apply dense to output state of every timestep\n",
    "# print (decoder_outputs.shape)\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([all_input, decoder_input], decoder_outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "encoder_model = Model(all_input, encoder_states)\n",
    "\n",
    "#we also need to train a decoder model, used for inference\n",
    "#the input for the decoder model is not only the summary, but also the initial context vector\n",
    "#in inference this is the states from the encoder, used at the inital vector for decoding\n",
    "decoder_state_inputs = [Input(shape=(HIDDEN_UNITS,)), Input(shape=(HIDDEN_UNITS,))]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_input, initial_state=decoder_state_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_input]+decoder_state_inputs, [decoder_outputs]+decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 8, 8, 64)\n",
      "(None, 8, 32)\n",
      "(None, None, 10000)\n",
      "[(None, 32), (None, 32), (None, 32)]\n",
      "[(None, None, 32), (None, 32), (None, 32)]\n",
      "(None, None, 10000)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print (layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           (None, 8, 8, 64)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 8, 32)        518994      input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           (None, None, 10000)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                  [(None, 32), (None,  8320        time_distributed_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm_12 (LSTM)                  [(None, None, 32), ( 1284224     input_16[0][0]                   \n",
      "                                                                 lstm_11[0][1]                    \n",
      "                                                                 lstm_11[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, None, 10000)  330000      lstm_12[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,141,538\n",
      "Trainable params: 2,141,538\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 8, 32)             518994    \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               [(None, 32), (None, 32),  8320      \n",
      "=================================================================\n",
      "Total params: 527,314\n",
      "Trainable params: 527,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           (None, None, 10000)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_12 (LSTM)                  [(None, None, 32), ( 1284224     input_16[0][0]                   \n",
      "                                                                 input_17[0][0]                   \n",
      "                                                                 input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, None, 10000)  330000      lstm_12[1][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,614,224\n",
      "Trainable params: 1,614,224\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(self, weight_file_path):\n",
    "    if os.path.exists(weight_file_path):\n",
    "        model.load_weights(weight_file_path)\n",
    "\n",
    "def get_weight_path(model_dir_path):\n",
    "    if not os.path.exists(model_dir_path):\n",
    "        os.makedirs(model_dir_path)\n",
    "    return model_dir_path + '/' + MODEL_NAME + '-weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # note the x_samples and y_samples are already tokenized and padded\n",
    "# # in the vocab should add in a <eos> \n",
    "# # decoder input: x y <eos>, decoder target: <sos> x y\n",
    "# def generate_batch(x_samples, y_samples, batch_size=1):\n",
    "#     num_batches = len(x_samples)//batch_size\n",
    "#     while True:\n",
    "#         for start in range(0, num_batches):\n",
    "#             encoder_input_batch = np.expand_dims(x_samples[start], 0)\n",
    "#             decoder_target_batch = np.expand_dims(y_samples[start], 0)\n",
    "#             decoder_input_batch = np.expand_dims(np.array([SOS_IDX] + list(y_samples[start][:-1])), 0)\n",
    "#             print ('encoder:', encoder_input_batch.shape)\n",
    "#             print ('decoder input:', decoder_input_batch.shape)\n",
    "#             print ('decoder target:', decoder_target_batch.shape)\n",
    "\n",
    "            \n",
    "#             yield [encoder_input_batch, decoder_input_batch], decoder_target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(x_samples, y_samples, batch_size):\n",
    "        num_batches = len(x_samples) // batch_size\n",
    "        while True:\n",
    "            for batchIdx in range(0, num_batches):\n",
    "                start = batchIdx * batch_size\n",
    "                end = (batchIdx + 1) * batch_size\n",
    "                encoder_input_batch = x_samples[start:end]\n",
    "                decoder_target_batch = np.zeros(shape=(batch_size, DECODER_MAX_LEN, MAX_NUM_WORDS))\n",
    "                decoder_input_batch = np.zeros(shape=(batch_size, DECODER_MAX_LEN, MAX_NUM_WORDS))\n",
    "                for lineIdx, target_words in enumerate(y_samples[start:end]):\n",
    "                    decoder_target_batch[lineIdx, 0, SOS_IDX] = 1\n",
    "                    for idx, w in enumerate(target_words):\n",
    "                        decoder_input_batch[lineIdx, idx, w] = 1\n",
    "                        if idx < len(target_words)-1:\n",
    "                            decoder_target_batch[lineIdx, idx+1, w] = 1\n",
    "                yield [encoder_input_batch, decoder_input_batch], decoder_target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can add more callback tricks like early stoppping etc\n",
    "def fit(Xtrain, Ytrain, val_data, epochs=EPOCHS, batch_size=BATCH_SIZE, model_dir_path=None):\n",
    "    (x_val, y_val) = val_data\n",
    "    if model_dir_path is None:\n",
    "        model_dir_path = \"./models\"\n",
    "    weight_file_path = get_weight_path(model_dir_path)\n",
    "    checkpoint = ModelCheckpoint(weight_file_path)\n",
    "    \n",
    "    train_gen = generate_batch(Xtrain, Ytrain, batch_size)\n",
    "    val_gen = generate_batch(x_val, y_val, batch_size)\n",
    "    \n",
    "    train_num_batches = len(Xtrain) // batch_size\n",
    "    val_num_batches = len(x_val) // batch_size\n",
    "    \n",
    "    \n",
    "    # can't do validation like that\n",
    "    # need to implement decoder for inference\n",
    "#     history = model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches,\n",
    "#                                   epochs=epochs, verbose=1, \n",
    "#                                   validation_data=val_gen, validation_steps=val_num_batches,\n",
    "#                                   callbacks=[checkpoint])\n",
    "    history = model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches,\n",
    "                                  epochs=epochs, verbose=1, validation_data=val_gen, \n",
    "                                  validation_steps=val_num_batches,\n",
    "                                  callbacks=[checkpoint])\n",
    "    model.save_weights(weight_file_path)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit(X_split, Y, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference decoder\n",
    "# the input is tokenized and padded validation/test data\n",
    "# input shape: (8, 8, encoder_max_len/64)\n",
    "\n",
    "# seems like it only decode one sentence at a time\n",
    "# because decoded sentences end at different time\n",
    "def summarize(input_seq):\n",
    "    input_seq = np.expand_dims(input_seq, 0)\n",
    "    state_value = encoder_model.predict(input_seq)\n",
    "    # prev predicted word\n",
    "    target_seq = np.zeros((1, 1, MAX_NUM_WORDS))\n",
    "    target_seq[0, 0, SOS_IDX] = 1\n",
    "    target_text = ''\n",
    "    target_text_len = 0\n",
    "    terminated = False \n",
    "    while not terminated:\n",
    "        # predict one word at a time,\n",
    "        # based on prev states and prev words\n",
    "        # output shape of decoder: decoder_outputs, state_h, state_c\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + state_value)\n",
    "        \n",
    "        sample_token_idx = np.argmax(output_tokens[0, -1, :])\n",
    "        sample_word = vocab_index_word[sample_token_idx]\n",
    "        target_text_len += 1\n",
    "        \n",
    "        if sample_word != \"<sos>\" and sample_word != \"<eos>\":\n",
    "            target_text += ' ' + sample_word \n",
    "        \n",
    "        if sample_word == \"<eos>\" or target_text_len >= DECODER_MAX_LEN:\n",
    "            terminated = True \n",
    "        \n",
    "        target_seq = np.zeros((1, 1, MAX_NUM_WORDS))\n",
    "        target_seq[0, 0, sample_token_idx] = 1\n",
    "        \n",
    "        state_values = [h, c]\n",
    "    # the returned value is a string\n",
    "    return target_text.strip()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from a sequence of indices to actual text,\n",
    "# so that we can evaluate\n",
    "def from_idx_to_text(seq):\n",
    "    text = [str(vocab_index_word[idx]) for idx in seq]\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rouge score\n",
    "# usage of rouge library: https://pypi.org/project/rouge/\n",
    "from rouge import Rouge\n",
    "# input hypothesis list and reference list\n",
    "def get_rouge(hyp, ref):\n",
    "    rouge = Rouge()\n",
    "    # avg of scores of all sentences\n",
    "    scores = rouge.get_scores(hyp, ref, avg=True)\n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_len = X_split.shape[0]\n",
    "val_split = 0.1\n",
    "test_split = 0.1\n",
    "x_train = X_split[: int(total_len*(1-val_split-test_split))]\n",
    "y_train = Y[: int(total_len*(1-val_split-test_split))]\n",
    "x_val = X_split[int(total_len*(1-val_split-test_split)) : int(total_len*(1-test_split))]\n",
    "y_val = Y[int(total_len*(1-val_split-test_split)) : int(total_len*(1-test_split))]\n",
    "x_test = X_split[int(total_len*(1-test_split)) :]\n",
    "y_test = Y[int(total_len*(1-test_split)) :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 200)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "8/8 [==============================] - 38s 5s/step - loss: 9.1892 - acc: 0.1864 - val_loss: 9.1101 - val_acc: 0.1795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sichenglei/Desktop/kaggle/salt/venv/lib/python3.6/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_12 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_11/while/Exit_3:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'lstm_11/while/Exit_4:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bbb005f8>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(x_train, y_train, val_data=(x_val, y_val), epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(get_weight_path(\"./models\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 8, 8, 64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_texts = []\n",
    "for text in x_test:\n",
    "    summarized_texts.append(summarize(text))\n",
    "# ref_texts = [from_idx_to_text(idx) for idx in y_test]\n",
    "# print (get_rouge(summarized_texts, ref_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '', '', '', '', '', '', '']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not enough training, so get <eos> every time\n",
    "summarized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_texts = [from_idx_to_text(idx) for idx in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"checked and was considered as control the dentin and pulp of extracted teeth were tested for the presence of abo rh antigen at respective time periods by ae technique statistical analysis data were analyzed in proportion for comparison chi square test or fisher 's exact test was used for the small sample results blood group antigens of abo and rh factor were detected in dentin and pulp up to 12 months for both abo and rh factor dentin and pulp showed 100 sensitivity for the samples tested at 0 month and showed a gradual decrease in the sensitivity as time period increased the sensitivity of pulp was better than dentin for both the blood grouping systems and abo blood group antigens were better detected than rh antigens conclusion in dentin and pulp the antigens of abo and rh factor were detected up to 12 months but showed a progressive decrease in the antigenicity as the time period increased when compared the results obtained of dentin and pulp in abo and rh factor grouping showed similar results with no statistical significance the sensitivity of abo blood grouping was better than rh factor blood grouping and showed a statistically significant result <eos>\""
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'f': 0.749999995, 'p': 0.75, 'r': 0.75}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.7499999999995, 'p': 0.75, 'r': 0.75}}\n"
     ]
    }
   ],
   "source": [
    "# so you can get the rouge score in this way\n",
    "hyp = [\"hello daniel\", \"yes\"]\n",
    "ref = [\"hi daniel\", \"yes\"]\n",
    "print (get_rouge(hyp, ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cannot get result yet because summarized texts cannot be empty\n",
    "print (get_rouge(summarized_texts, ref_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
